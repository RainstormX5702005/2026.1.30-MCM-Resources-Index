import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from typing import Tuple, Dict

from configs.config import OUTPUT_DIR

# é…ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„
INPUT_FILE = OUTPUT_DIR / "question2_res" / "wilcoxon" / "wilcoxon_full_data.csv"


def load_and_validate_data(file_path: str) -> pd.DataFrame:
    """
    åŠ è½½æ•°æ®å¹¶è¿›è¡ŒåŸºç¡€æ ¡éªŒ
    """
    print(f"Loading data from {file_path}...")
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}ã€‚è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ã€‚")

    # å¿…è¦çš„åˆ—æ£€æŸ¥
    required_columns = ["season", "week", "method1_pos", "method2_pos", "placement"]
    missing = [c for c in required_columns if c not in df.columns]
    if missing:
        raise ValueError(f"æ•°æ®é”™è¯¯: ç¼ºå°‘å…³é”®åˆ— {missing}ã€‚è¯·æ£€æŸ¥ CSV æ–‡ä»¶å¤´ã€‚")

    # ç±»å‹è½¬æ¢ä¸æ¸…æ´—
    for col in required_columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    # åˆ é™¤å«æœ‰ç©ºå€¼çš„è¡Œ (ä¿è¯å¯¹æ¯”å…¬å¹³)
    initial_len = len(df)
    df = df.dropna(subset=required_columns).copy()
    print(f"Data loaded. Rows: {len(df)} (Dropped {initial_len - len(df)} NaN rows)")

    return df


def prepare_ground_truth(df: pd.DataFrame) -> pd.DataFrame:
    """
    å‡†å¤‡çœŸå®æ’åï¼ˆGround Truthï¼‰

    ä½¿ç”¨æœ€ç»ˆåæ¬¡ placement ä½œä¸ºçœŸå®æ’åï¼š
    - placement è¶Šå°è¶Šå¥½ï¼ˆ1st æœ€å¥½ï¼‰
    - åœ¨æ¯å‘¨å†…ï¼Œæ ¹æ® placement å»ºç«‹çœŸå®çš„ä¼˜åŠ£é¡ºåº
    """
    df = df.copy()

    # åœ¨æ¯å‘¨å†…ï¼Œæ ¹æ® placement è®¡ç®—çœŸå®æ’å
    # placement å°çš„åº”è¯¥æ’åé å‰ï¼ˆå€¼å°ï¼‰
    df["true_rank"] = df.groupby(["season", "week"])["placement"].rank(
        ascending=True, method="average"
    )

    return df


def calculate_weekly_correlation(
    df: pd.DataFrame,
    method_col: str,
    truth_col: str = "true_rank",
    min_contestants: int = 4,
) -> pd.DataFrame:
    """
    è®¡ç®—æ¯ä¸€å‘¨çš„ Spearman ç›¸å…³ç³»æ•°

    Args:
        df: æ•°æ®æ¡†
        method_col: å¾…è¯„ä¼°çš„æ–¹æ³•æ’ååˆ— (ä¾‹å¦‚ 'method1_pos' æˆ– 'method2_pos')
        truth_col: çœŸå®æ’ååˆ— (é»˜è®¤ä¸º 'true_rank')
        min_contestants: æ¯å‘¨æœ€å°‘å‚èµ›äººæ•° (å°‘äºæ­¤æ•°ä¸è®¡ç®—)

    Returns:
        åŒ…å«æ¯å‘¨ correlation (rho) çš„ DataFrame
    """
    results = []

    # æŒ‰èµ›å­£å’Œå‘¨åˆ†ç»„éå†
    for (season, week), group in df.groupby(["season", "week"]):
        # å†æ¬¡ç¡®ä¿æ— ç©ºå€¼
        valid_data = group[[method_col, truth_col]].dropna()
        n = len(valid_data)

        if n < min_contestants:
            continue

        # è®¡ç®— Spearman ç›¸å…³ç³»æ•°
        rho, p_value = spearmanr(valid_data[method_col], valid_data[truth_col])

        results.append(
            {
                "season": season,
                "week": week,
                "n_contestants": n,
                "rho": rho,
                "p_value": p_value,
            }
        )

    return pd.DataFrame(results)


def compare_methods(
    df_corr_a: pd.DataFrame, df_corr_b: pd.DataFrame
) -> Tuple[Dict, pd.DataFrame]:
    """
    å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç›¸å…³ç³»æ•°ç»“æœ
    """
    # åˆå¹¶ä¸¤ç»„ç»“æœï¼Œç¡®ä¿åªæ¯”è¾ƒåŒä¸€å‘¨çš„æ•°æ® (å¯¹é½)
    merged = pd.merge(
        df_corr_a, df_corr_b, on=["season", "week"], suffixes=("_rank", "_share")
    )

    # è®¡ç®—å·®å¼‚ (Shareæ¨¡å‹ - Rankæ¨¡å‹)
    # rho è¶Šé«˜è¶Šå¥½ (è¶Šæ¥è¿‘ 1 è¯´æ˜è¶Šå‡†ç¡®)
    merged["diff"] = merged["rho_share"] - merged["rho_rank"]

    stats = {
        "n_weeks_compared": len(merged),
        "mean_rho_rank": merged["rho_rank"].mean(),
        "mean_rho_share": merged["rho_share"].mean(),
        "mean_diff": merged["diff"].mean(),
        "median_diff": merged["diff"].median(),
        "win_rate_rank": (merged["diff"] < 0).mean(),  # Rank æ¨¡å‹èƒœå‡ºçš„æ¯”ä¾‹
        "win_rate_share": (merged["diff"] > 0).mean(),  # Share æ¨¡å‹èƒœå‡ºçš„æ¯”ä¾‹
        # p å€¼ç»Ÿè®¡
        "mean_p_rank": merged["p_value_rank"].mean(),
        "mean_p_share": merged["p_value_share"].mean(),
        "significant_rank": (merged["p_value_rank"] < 0.05).mean(),  # æ˜¾è‘—ç›¸å…³æ¯”ä¾‹
        "significant_share": (merged["p_value_share"] < 0.05).mean(),  # æ˜¾è‘—ç›¸å…³æ¯”ä¾‹
        "very_significant_rank": (merged["p_value_rank"] < 0.01).mean(),  # é«˜åº¦æ˜¾è‘—æ¯”ä¾‹
        "very_significant_share": (
            merged["p_value_share"] < 0.01
        ).mean(),  # é«˜åº¦æ˜¾è‘—æ¯”ä¾‹
    }

    return stats, merged


def print_report(stats: Dict):
    """
    æ‰“å°æ¼‚äº®çš„åˆ†ææŠ¥å‘Š
    """
    print("\n" + "=" * 50)
    print("ğŸ†  MODEL COMPARISON REPORT: SHARE vs RANK")
    print("=" * 50)
    print(f"Total Weeks Analyzed: {stats['n_weeks_compared']}")
    print("-" * 30)
    print(f"1. Average Correlation (Higher is Better):")
    print(f"   - Share Model (Ours): {stats['mean_rho_share']:.4f}")
    print(f"   - Rank Model (Official): {stats['mean_rho_rank']:.4f}")
    print("-" * 30)
    print(f"2. Statistical Significance (p-values):")
    print(f"   - Share Model Mean p-value: {stats['mean_p_share']:.6f}")
    print(f"   - Rank Model Mean p-value:  {stats['mean_p_rank']:.6f}")
    print(
        f"   - Share Model Significant (p<0.05): {stats['significant_share']*100:.1f}%"
    )
    print(
        f"   - Rank Model Significant (p<0.05):  {stats['significant_rank']*100:.1f}%"
    )
    print(
        f"   - Share Model Very Significant (p<0.01): {stats['very_significant_share']*100:.1f}%"
    )
    print(
        f"   - Rank Model Very Significant (p<0.01):  {stats['very_significant_rank']*100:.1f}%"
    )
    print("-" * 30)
    print(f"3. Direct Head-to-Head Comparison:")
    print(f"   - Mean Improvement: {stats['mean_diff']:.4f}")
    print(f"   - Median Improvement: {stats['median_diff']:.4f}")
    print("-" * 30)
    print(f"4. Win Rate (Which model was more accurate per week?):")
    print(f"   - Share Model Wins: {stats['win_rate_share']*100:.1f}%")
    print(f"   - Rank Model Wins:  {stats['win_rate_rank']*100:.1f}%")
    print("=" * 50)

    if stats["mean_diff"] > 0:
        print("âœ… CONCLUSION: The Share-based model is more accurate.")
        print("   It better reflects the true contestant rankings.")
    else:
        print("âŒ CONCLUSION: The Rank-based model is more accurate.")


def main():
    """æ‰§è¡Œ Spearman ç›¸å…³æ€§åˆ†æ"""
    # 1. åŠ è½½æ•°æ®
    try:
        df = load_and_validate_data(INPUT_FILE)
    except Exception as e:
        print(e)
        return

    # 2. å‡†å¤‡çœŸå®æ’å
    print("\nPreparing ground truth rankings...")
    df = prepare_ground_truth(df)
    print(f"  Ground truth based on final placement")

    # 3. è®¡ç®—æ¯ç§æ–¹æ³•çš„æ¯å‘¨è¡¨ç°
    print("\nCalculating correlations for Rank Model (Method 1)...")
    corr_rank = calculate_weekly_correlation(
        df, method_col="method1_pos", truth_col="true_rank"
    )

    print("Calculating correlations for Share Model (Method 2)...")
    corr_share = calculate_weekly_correlation(
        df, method_col="method2_pos", truth_col="true_rank"
    )

    # 4. å¯¹æ¯”ä¸¤ç§æ–¹æ³•
    print("Comparing methods...")
    stats, comparison_df = compare_methods(corr_rank, corr_share)

    # 5. è¾“å‡ºæŠ¥å‘Š
    print_report(stats)

    # 6. ä¿å­˜è¯¦ç»†å¯¹æ¯”ç»“æœ
    output_dir = OUTPUT_DIR / "question2_res" / "spearman"

    corr_rank.to_csv(
        output_dir / "spearman_rank_method.csv", index=False, encoding="utf-8"
    )
    corr_share.to_csv(
        output_dir / "spearman_share_method.csv", index=False, encoding="utf-8"
    )
    comparison_df.to_csv(
        output_dir / "spearman_comparison.csv", index=False, encoding="utf-8"
    )

    print(f"\nâœ“ Detailed results saved:")
    print(f"  - spearman_rank_method.csv")
    print(f"  - spearman_share_method.csv")
    print(f"  - spearman_comparison.csv")


if __name__ == "__main__":
    main()
