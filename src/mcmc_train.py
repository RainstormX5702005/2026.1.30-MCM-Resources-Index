"""
è´å¶æ–¯å±‚æ¬¡æ¨¡å‹ + MCMC æ¨æ–­ç²‰ä¸æŠ•ç¥¨å¼ºåº¦
å®Œæ•´ç‰ˆæœ¬ - ä½¿ç”¨ C ç¼–è¯‘åŠ é€Ÿï¼Œå¤šæ ¸å¹¶è¡Œé‡‡æ ·
èåˆç‰ˆæœ¬ï¼šç»“åˆå¯è§†åŒ–è¯Šæ–­ä¸æ·˜æ±°é¢„æµ‹
"""

import os
import warnings

os.environ.setdefault(
    "PYTENSOR_FLAGS", "device=cpu,floatX=float64,optimizer=fast_compile"
)

# å±è”½éå…³é”®è­¦å‘Š
warnings.simplefilter(action="ignore", category=FutureWarning)

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
import multiprocessing as mp
from typing import Dict, List, Tuple
from dataclasses import dataclass

from configs.config import OUTPUT_DIR

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)


@dataclass
class MCMCConfig:
    """MCMC é‡‡æ ·é…ç½®"""

    draws: int = 1000
    tune: int = 1000
    chains: int = 4
    cores: int = -1
    target_accept: float = 0.95
    init: str = "jitter+adapt_diag"  # æ›´ç¨³å®šçš„åˆå§‹åŒ–æ–¹æ³•

    def __post_init__(self):
        if self.cores == -1:
            self.cores = min(mp.cpu_count(), self.chains)


def load_preprocessed_data(
    file_name: str = "preprocessed_data_percentage.csv",
) -> pd.DataFrame:
    """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
    file_path = OUTPUT_DIR / "preprocessed" / file_name

    try:
        df = pd.read_csv(file_path, encoding="utf-8")
        return df
    except FileNotFoundError as e:
        raise


def prepare_indices(df: pd.DataFrame) -> Tuple[Dict, int, int]:
    """
    å‡†å¤‡èµ›å­£å’Œé€‰æ‰‹ç´¢å¼•

    Returns:
        (season_map, n_seasons, n_contestants)
    """
    seasons = sorted(df["season"].unique())
    season_map = {s: i for i, s in enumerate(seasons)}

    df["season_idx"] = df["season"].map(season_map)
    df["contestant_id"] = range(len(df))

    n_seasons = len(seasons)
    n_contestants = len(df)

    return season_map, n_seasons, n_contestants


def extract_features(
    df: pd.DataFrame, n_contestants: int
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ç›´æ¥ä»é¢„å¤„ç†æ•°æ®ä¸­æå–ç‰¹å¾ï¼ˆä¸é‡å¤ç¼–ç ï¼‰

    Returns:
        (X_industry, X_age, X_advanced_rounds)
    """
    industry_cols = [c for c in df.columns if "celebrity_industry_" in c]
    if len(industry_cols) > 0:
        X_industry = df[industry_cols].values.astype(np.float64)
    else:
        X_industry = np.zeros((n_contestants, 1), dtype=np.float64)

    if "celebrity_age_during_season" in df.columns:
        X_age = df["celebrity_age_during_season"].values.astype(np.float64)
    else:
        X_age = np.zeros(n_contestants, dtype=np.float64)

    # æå–æ™‹çº§è½®æ¬¡ï¼ˆæ ‡å‡†åŒ–åï¼‰ä½œä¸ºäººæ°”ç‰¹å¾
    if "advanced_rounds" in df.columns:
        X_advanced_rounds = df["advanced_rounds"].values.astype(np.float64)
    else:
        X_advanced_rounds = np.zeros(n_contestants, dtype=np.float64)

    return X_industry, X_age, X_advanced_rounds


def build_observation_data(
    df: pd.DataFrame,
    max_weeks: int = 11,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict]:
    """
    æ„å»ºé•¿æ ¼å¼è§‚æµ‹æ•°æ®

    Returns:
        (obs_season_idx, obs_week_idx, obs_contestant_idx, obs_score_sum, flat_idx_map)
    """
    obs_season_idx = []
    obs_week_idx = []
    obs_contestant_idx = []
    obs_score_sum = []  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ†æ•°æ€»å’Œ
    flat_idx_map = {}
    current_flat_idx = 0

    for idx, row in df.iterrows():
        c_id = row["contestant_id"]
        s_idx = row["season_idx"]

        # è·å–é€‰æ‰‹å®é™…å‚ä¸çš„å‘¨æ•°
        weeks_participated = None
        if "weeks_participated" in df.columns:
            wp = row.get("weeks_participated", np.nan)
            try:
                weeks_participated = int(wp) if pd.notna(wp) else None
            except (ValueError, TypeError):
                weeks_participated = None

        for w in range(1, max_weeks + 1):
            col_score = f"week{w}_score_sum"  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ†æ•°æ€»å’Œ

            if col_score not in df.columns:
                continue

            score_val = row[col_score]

            # åˆ¤æ–­è¯¥å‘¨æ˜¯å¦æ˜¯åˆç†æ•°æ®ï¼šåŸºäºå®é™…å‚ä¸å‘¨æ•°
            participated = (
                weeks_participated is not None and w <= weeks_participated
            ) or (
                weeks_participated is None and pd.notna(score_val)
            )  # å›é€€å…¼å®¹

            if participated:
                # å³ä½¿ score_val ä¸º 0 ä¹Ÿæ˜¯æœ‰æ•ˆè§‚æµ‹ï¼ˆæ ‡å‡†åŒ–åå¯èƒ½ä¸º0ï¼‰
                obs_season_idx.append(s_idx)
                obs_week_idx.append(w - 1)
                obs_contestant_idx.append(c_id)
                obs_score_sum.append(score_val if pd.notna(score_val) else 0.0)
                flat_idx_map[(c_id, w)] = current_flat_idx
                current_flat_idx += 1

    return (
        np.array(obs_season_idx, dtype=np.int32),
        np.array(obs_week_idx, dtype=np.int32),
        np.array(obs_contestant_idx, dtype=np.int32),
        np.array(obs_score_sum, dtype=np.float64),  # è¿”å›æ ‡å‡†åŒ–åˆ†æ•°
        flat_idx_map,
    )


def build_elimination_pairs(
    df: pd.DataFrame,
    season_map: Dict,
    flat_idx_map: Dict,
    max_weeks: int = 11,
) -> Tuple[np.ndarray, Dict]:
    """
    æ„å»ºæ·˜æ±°çº¦æŸé…å¯¹ï¼ˆä¸¥æ ¼é¿å…ä¿¡æ¯æ³„éœ²ï¼‰

    æ”¹è¿›é€»è¾‘ï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰ï¼š
    1. **ä¸ä¾èµ– weeks_participated**ï¼ˆè¿™æ˜¯æœªæ¥ä¿¡æ¯ï¼ï¼‰
    2. **ä¸ä¾èµ– placement**ï¼ˆæœ€ç»ˆæ’åä¹Ÿæ˜¯æœªæ¥ä¿¡æ¯ï¼‰
    3. **åªçœ‹å½“å‰å‘¨å’Œä¸‹ä¸€å‘¨çš„æ•°æ®å­˜åœ¨æ€§**ï¼š
       - å¦‚æœé€‰æ‰‹åœ¨ç¬¬wå‘¨æœ‰æ•°æ®ï¼Œä½†åœ¨ç¬¬w+1å‘¨æ²¡æœ‰æ•°æ® â†’ ç¬¬wå‘¨è¢«æ·˜æ±°
       - å¦‚æœé€‰æ‰‹åœ¨ç¬¬wå‘¨å’Œç¬¬w+1å‘¨éƒ½æœ‰æ•°æ® â†’ ç¬¬wå‘¨æ™‹çº§
    4. çº¦æŸï¼šåœ¨åŒä¸€å‘¨å†…ï¼Œæ™‹çº§è€…çš„ç»¼åˆå¾—åˆ† > è¢«æ·˜æ±°è€…

    è¿™æ ·æ¨¡å‹åœ¨æ¨æ–­ç¬¬wå‘¨æ—¶ï¼Œåªèƒ½çœ‹åˆ°ç¬¬wå‘¨å’Œç¬¬w+1å‘¨çš„"æ˜¯å¦ç»§ç»­"ä¿¡æ¯ï¼Œ
    è€Œä¸ä¼šæå‰çŸ¥é“é€‰æ‰‹ä¼šå‚åŠ å¤šå°‘å‘¨æˆ–æœ€ç»ˆæ’åã€‚

    Returns:
        (elimination_pairs, pair_info)
        - elimination_pairs: [[winner_idx, loser_idx], ...]
        - pair_info: {pair_idx: {"winner": name, "loser": name, "week": w, "season": s}}
    """
    elimination_pairs = []
    pair_info = {}  # ç”¨äºè°ƒè¯•éªŒè¯
    pair_idx = 0

    for s in df["season"].unique():
        s_df = df[df["season"] == s]
        season_total_weeks = s_df["season_total_weeks"].iloc[0]

        # éå†æ¯ä¸€å‘¨ï¼ˆé™¤äº†æœ€åä¸€å‘¨ï¼Œå› ä¸ºæœ€åä¸€å‘¨æ²¡æœ‰æ·˜æ±°ï¼‰
        for w in range(1, min(max_weeks, season_total_weeks)):
            week_contestants = []

            for _, row in s_df.iterrows():
                c_id = row["contestant_id"]

                # æ£€æŸ¥æœ¬å‘¨æ˜¯å¦æœ‰è§‚æµ‹æ•°æ®
                if (c_id, w) not in flat_idx_map:
                    continue

                flat_idx = flat_idx_map[(c_id, w)]

                # **å…³é”®æ”¹è¿›**ï¼šåªçœ‹ä¸‹ä¸€å‘¨æ˜¯å¦æœ‰æ•°æ®ï¼ˆä¸çœ‹weeks_participatedï¼‰
                has_next_week = (c_id, w + 1) in flat_idx_map

                week_contestants.append(
                    {
                        "flat_idx": flat_idx,
                        "has_next_week": has_next_week,  # æ˜¯å¦æ™‹çº§ï¼ˆäºŒå€¼åŒ–ï¼‰
                        "contestant_id": c_id,
                        "name": row["celebrity_name"],
                    }
                )

            advanced = []  # æ™‹çº§è€…ï¼šä¸‹å‘¨æœ‰æ•°æ®
            eliminated = []  # æ·˜æ±°è€…ï¼šä¸‹å‘¨æ²¡æ•°æ®

            for c in week_contestants:
                if c["has_next_week"]:
                    advanced.append(c)
                else:
                    eliminated.append(c)

            # ç”Ÿæˆé…å¯¹ï¼šæ¯ä¸ªæ™‹çº§è€… vs æ¯ä¸ªè¢«æ·˜æ±°è€…
            for winner in advanced:
                for loser in eliminated:
                    elimination_pairs.append([winner["flat_idx"], loser["flat_idx"]])
                    pair_info[pair_idx] = {
                        "winner": winner["name"],
                        "loser": loser["name"],
                        "week": w,
                        "season": s,
                        "winner_continues": True,
                        "loser_continues": False,
                    }
                    pair_idx += 1

    return (
        (
            np.array(elimination_pairs, dtype=np.int32)
            if elimination_pairs
            else np.array([], dtype=np.int32).reshape(0, 2)
        ),
        pair_info,
    )


def build_pymc_model(
    obs_season_idx: np.ndarray,
    obs_week_idx: np.ndarray,
    obs_contestant_idx: np.ndarray,
    obs_score_sum: np.ndarray,
    X_industry: np.ndarray,
    X_age: np.ndarray,
    X_advanced_rounds: np.ndarray,
    elimination_pairs: np.ndarray,
    n_seasons: int,
    n_contestants: int,
    n_observations: int,
) -> pm.Model:
    """
    æ„å»ºå®Œæ•´çš„è´å¶æ–¯å±‚æ¬¡æ¨¡å‹

    æ¨¡å‹ç»“æ„ï¼š
    - season_trend: èµ›å­£è¶‹åŠ¿ (Gaussian Random Walk)
    - beta_week: å‘¨æ¬¡æ•ˆåº”
    - alpha: é€‰æ‰‹åŸºç¡€äººæ°”ï¼ˆèå…¥æ™‹çº§è½®æ¬¡ï¼‰
    - beta_judge: è¯„å§”åˆ†æƒé‡
    - beta_industry: èŒä¸šç‰¹å¾æƒé‡
    - beta_age: å¹´é¾„æƒé‡
    - V_latent: æ½œåœ¨æŠ•ç¥¨å¼ºåº¦ (Gamma åˆ†å¸ƒ)
    - constraint: æ·˜æ±°çº¦æŸ (Bernoulli)
    """
    n_industry_features = X_industry.shape[1]
    n_pairs = len(elimination_pairs)

    with pm.Model() as model:

        # 1. èµ›å­£è¶‹åŠ¿ (Gaussian Random Walk) - ä¼˜åŒ–ï¼šå‡å°æ–¹å·®
        sigma_season = pm.HalfNormal("sigma_season", sigma=0.05)
        season_trend = pm.GaussianRandomWalk(
            "season_trend",
            sigma=sigma_season,
            shape=n_seasons,
            init_dist=pm.Normal.dist(0, 0.05),
        )

        beta_week = pm.Normal("beta_week", mu=0, sigma=0.1)

        # 3. é€‰æ‰‹åŸºç¡€äººæ°”ï¼ˆèå…¥æ™‹çº§è½®æ¬¡ï¼Œå·²æ ‡å‡†åŒ–ï¼‰
        theta = pm.Normal("theta", mu=0, sigma=0.2)  # åŸºç¡€äººæ°”å‡å€¼
        theta_popularity = pm.Normal(
            "theta_popularity", mu=0.3, sigma=0.12
        )  # æ™‹çº§è½®æ¬¡æ•ˆåº”ç³»æ•°
        sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=0.5)

        # alphaå…ˆéªŒå‡å€¼ç”±æ™‹çº§è½®æ¬¡è°ƒæ•´ï¼šæ™‹çº§è½®æ¬¡è¶Šå¤šï¼Œäººæ°”è¶Šé«˜
        alpha_mu = theta + theta_popularity * X_advanced_rounds
        alpha = pm.Normal(
            "alpha",
            mu=alpha_mu,
            sigma=sigma_alpha,
            shape=n_contestants,
        )

        # 4. è¯„å§”åˆ†æƒé‡
        beta_judge = pm.Normal("beta_judge", mu=0.5, sigma=0.3)

        # 5. èŒä¸šç‰¹å¾æƒé‡
        beta_ind = pm.Normal("beta_ind", mu=0, sigma=0.3, shape=n_industry_features)

        # 6. å¹´é¾„æƒé‡
        beta_age = pm.Normal("beta_age", mu=0, sigma=0.8)

        # === Log-Linear æ¨¡å‹ï¼ˆæŠ•ç¥¨å¼ºåº¦ï¼‰ ===
        log_mu = (
            alpha[obs_contestant_idx]
            + beta_judge * obs_score_sum
            + pm.math.dot(X_industry, beta_ind)[obs_contestant_idx]
            + beta_age * X_age[obs_contestant_idx]
            + season_trend[obs_season_idx]
            + beta_week * obs_week_idx
        )

        phi = pm.HalfNormal("phi", sigma=2.0)
        mu_ = pm.math.exp(log_mu)
        V_latent = pm.Gamma(
            "V_latent",
            alpha=phi,
            beta=phi / mu_,
            shape=n_observations,
        )

        # === æ·˜æ±°çº¦æŸ ===
        if n_pairs > 0:
            winners_idx = elimination_pairs[:, 0]
            losers_idx = elimination_pairs[:, 1]

            # çº¦æŸï¼šæ™‹çº§è€…çš„ç»¼åˆå¾—åˆ† > æ·˜æ±°è€… - ä¼˜åŒ–ï¼šå‡å°çº¦æŸå¼ºåº¦
            diff = (obs_score_sum[winners_idx] - obs_score_sum[losers_idx]) + 0.3 * (
                pt.log(V_latent[winners_idx]) - pt.log(V_latent[losers_idx])
            )

            # Sigmoid æ¦‚ç‡çº¦æŸ - ä¼˜åŒ–ï¼šä»5é™è‡³3
            p_outcome = pm.math.sigmoid(diff * 3)
            pm.Bernoulli(
                "constraint",
                p=p_outcome,
                observed=np.ones(n_pairs, dtype=np.int32),
            )

    return model


def run_mcmc_sampling(model: pm.Model, config: MCMCConfig) -> az.InferenceData:
    """è¿è¡Œ MCMC é‡‡æ ·"""
    print(f"ğŸš€ Starting MCMC sampling with {config.cores} cores...")
    print(f"   Chains: {config.chains}, Draws: {config.draws}, Tune: {config.tune}")
    print(f"   Init: {config.init}")

    with model:
        trace = pm.sample(
            draws=config.draws,
            tune=config.tune,
            chains=config.chains,
            cores=config.cores,
            target_accept=config.target_accept,
            init=config.init,
            return_inferencedata=True,
            progressbar=True,
            idata_kwargs={"log_likelihood": False},  # å‡å°‘å†…å­˜ä½¿ç”¨
            compute_convergence_checks=False,  # ç¦ç”¨è‡ªåŠ¨æ”¶æ•›æ£€æŸ¥ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åš
        )

    return trace


def extract_results(
    trace: az.InferenceData,
    df: pd.DataFrame,
    obs_season_idx: np.ndarray,
    obs_week_idx: np.ndarray,
    obs_contestant_idx: np.ndarray,
    obs_score_sum: np.ndarray,
    season_map: Dict,
) -> pd.DataFrame:
    """æå–æ¨æ–­ç»“æœ"""
    # æå–æ½œåœ¨ç¥¨æ•°åéªŒ
    if "V_latent" not in trace.posterior:
        raise ValueError(
            "V_latent not found in trace.posterior. Available variables: "
            + ", ".join(trace.posterior.data_vars.keys())
        )

    v_samples = trace.posterior["V_latent"].values  # (chains, draws, observations)

    # æ£€æŸ¥æ•°ç»„å¤§å°
    if v_samples.size == 0:
        raise ValueError(f"V_latent samples array is empty. Shape: {v_samples.shape}")

    v_mean = v_samples.mean(axis=(0, 1))
    v_std = v_samples.std(axis=(0, 1))
    v_lower = np.percentile(v_samples, 2.5, axis=(0, 1))
    v_upper = np.percentile(v_samples, 97.5, axis=(0, 1))

    # åè½¬ season_map
    inv_season_map = {v: k for k, v in season_map.items()}

    # æ„é€ ç»“æœè¡¨
    results = []
    for i in range(len(obs_score_sum)):
        c_idx = obs_contestant_idx[i]
        celeb_name = df.loc[df["contestant_id"] == c_idx, "celebrity_name"].values[0]

        results.append(
            {
                "season": inv_season_map[obs_season_idx[i]],
                "week": obs_week_idx[i] + 1,
                "celebrity_name": celeb_name,
                "contestant_id": c_idx,
                "judge_score_sum": obs_score_sum[i],
                "vote_intensity_mean": v_mean[i],
                "vote_intensity_std": v_std[i],
                "vote_intensity_lower_95": v_lower[i],
                "vote_intensity_upper_95": v_upper[i],
            }
        )

    result_df = pd.DataFrame(results)

    # æ’åº
    result_df = result_df.sort_values(
        ["season", "week", "vote_intensity_mean"],
        ascending=[True, True, False],
    ).reset_index(drop=True)

    return result_df


def save_results(df: pd.DataFrame, output_file: str) -> None:
    """ä¿å­˜ç»“æœ"""
    output_path = OUTPUT_DIR / "trained" / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)

    df.to_csv(output_path, index=False)
    print(f"âœ… Results saved to: {output_path}")


def export_elimination_analysis_to_excel(
    df: pd.DataFrame,
    flat_idx_map: Dict,
    pair_info: Dict,
    output_file: str = "elimination_analysis.xlsx",
) -> None:
    """
    å¯¼å‡ºæ¯ä¸ªèµ›å­£çš„æ·˜æ±°/æ™‹çº§åˆ†æåˆ°Excel
    æ¯ä¸ªseasonä¸€ä¸ªsheetï¼ŒåŒ…å«æ¯å‘¨çš„é€‰æ‰‹çŠ¶æ€
    """
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment

    output_path = OUTPUT_DIR / "trained" / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºExcel writer
    writer = pd.ExcelWriter(output_path, engine="openpyxl")

    # æŒ‰èµ›å­£åˆ†ç»„
    for season in sorted(df["season"].unique()):
        s_df = df[df["season"] == season].copy()
        season_total_weeks = s_df["season_total_weeks"].iloc[0]

        # æ„å»ºæ¯å‘¨çš„é€‰æ‰‹çŠ¶æ€è¡¨
        weekly_data = []

        for week in range(1, min(12, season_total_weeks + 1)):
            for _, row in s_df.iterrows():
                c_id = row["contestant_id"]
                c_name = row["celebrity_name"]

                # æ£€æŸ¥æœ¬å‘¨æ˜¯å¦æœ‰æ•°æ®
                has_this_week = (c_id, week) in flat_idx_map
                has_next_week = (c_id, week + 1) in flat_idx_map

                if has_this_week:
                    # è·å–æœ¬å‘¨çš„æ ‡å‡†åŒ–åˆ†æ•°
                    week_col = f"week{week}_score_sum"
                    judge_score = row.get(week_col, None)

                    # åˆ¤æ–­çŠ¶æ€
                    if has_next_week:
                        status = "æ™‹çº§"
                        status_en = "Advanced"
                    elif week == season_total_weeks:
                        status = "å† å†›"
                        status_en = "Winner"
                    else:
                        status = "æ·˜æ±°"
                        status_en = "Eliminated"

                    weekly_data.append(
                        {
                            "Week": week,
                            "Celebrity": c_name,
                            "Judge_Score_Sum": (  # æ”¹ä¸ºæ ‡å‡†åŒ–åˆ†æ•°
                                judge_score if pd.notna(judge_score) else 0
                            ),
                            "Status": status,
                            "Status_EN": status_en,
                            "Has_Next_Week": has_next_week,
                            "Advanced_Rounds": row.get("advanced_rounds", 0),
                            "Total_Weeks_Participated": row["weeks_participated"],
                            "Final_Placement": row["placement"],
                        }
                    )

        # åˆ›å»ºDataFrame
        weekly_df = pd.DataFrame(weekly_data)

        if len(weekly_df) > 0:
            # æ’åºï¼šæŒ‰å‘¨æ¬¡ã€çŠ¶æ€ã€è¯„å§”åˆ†ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åˆ†æ•°æ€»å’Œï¼‰
            weekly_df = weekly_df.sort_values(
                ["Week", "Status", "Judge_Score_Sum"], ascending=[True, False, False]
            )

            # å†™å…¥Excel
            sheet_name = f"Season_{season}"
            weekly_df.to_excel(writer, sheet_name=sheet_name, index=False)

            # æ ¼å¼åŒ–ï¼ˆæ·»åŠ é¢œè‰²æ ‡è®°ï¼‰
            worksheet = writer.sheets[sheet_name]

            # è®¾ç½®åˆ—å®½
            worksheet.column_dimensions["A"].width = 8
            worksheet.column_dimensions["B"].width = 15
            worksheet.column_dimensions["C"].width = 18
            worksheet.column_dimensions["D"].width = 12
            worksheet.column_dimensions["E"].width = 12

            # è®¾ç½®è¡¨å¤´æ ·å¼
            header_fill = PatternFill(
                start_color="366092", end_color="366092", fill_type="solid"
            )
            header_font = Font(color="FFFFFF", bold=True)

            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center")

            # æ ¹æ®çŠ¶æ€è®¾ç½®è¡Œé¢œè‰²
            advanced_fill = PatternFill(
                start_color="C6EFCE", end_color="C6EFCE", fill_type="solid"
            )  # ç»¿è‰²
            eliminated_fill = PatternFill(
                start_color="FFC7CE", end_color="FFC7CE", fill_type="solid"
            )  # çº¢è‰²
            winner_fill = PatternFill(
                start_color="FFEB9C", end_color="FFEB9C", fill_type="solid"
            )  # é‡‘è‰²

            for row_idx, row in enumerate(
                worksheet.iter_rows(min_row=2, max_row=len(weekly_df) + 1), start=2
            ):
                status = worksheet.cell(row_idx, 4).value

                if status == "æ™‹çº§":
                    fill = advanced_fill
                elif status == "æ·˜æ±°":
                    fill = eliminated_fill
                elif status == "å† å†›":
                    fill = winner_fill
                else:
                    fill = None

                if fill:
                    for cell in row:
                        cell.fill = fill
                        cell.alignment = Alignment(horizontal="center")

    # ä¿å­˜
    writer.close()
    print(f"âœ… Elimination analysis saved to: {output_path}")


def analyze_and_visualize_results(
    trace: az.InferenceData,
    result_df: pd.DataFrame,
    df: pd.DataFrame,
    season_map: Dict,
    output_dir_name: str = "mcmc_figures",
) -> None:
    """
    æ¨¡å‹è¯Šæ–­ã€å¯è§†åŒ–ä¸ç»“æœåˆ†æ
    å€Ÿé‰´è‡ª è´å¶æ–¯åˆ†å±‚+mcmc.py çš„ analyze_results å‡½æ•°
    """
    output_dir = OUTPUT_DIR / "trained" / output_dir_name
    output_dir.mkdir(parents=True, exist_ok=True)

    print("\n" + "=" * 60)
    print("ğŸ“Š Model Diagnostics & Visualization")
    print("=" * 60)

    # 1. è¯Šæ–­ç»Ÿè®¡é‡ - å…³é”®å‚æ•°æ‘˜è¦
    print("\n[1] Key Parameter Summary:")
    try:
        summary = az.summary(
            trace,
            var_names=[
                "sigma_season",
                "beta_week",
                "beta_judge",
                "theta",
                "theta_popularity",
                "sigma_alpha",
                "phi",
            ],
        )
        print(summary)
        # ä¿å­˜æ‘˜è¦åˆ°CSV
        summary.to_csv(output_dir / "parameter_summary.csv")
        print(f"   Saved to: {output_dir / 'parameter_summary.csv'}")
    except Exception as e:
        print(f"   Warning: Could not generate summary - {e}")

    # 2. è½¨è¿¹å›¾ (Traceplot)
    print("\n[2] Generating Traceplot...")
    try:
        fig, axes = plt.subplots(4, 2, figsize=(14, 12))
        az.plot_trace(
            trace,
            var_names=["beta_judge", "beta_week", "theta", "theta_popularity"],
            compact=True,
            axes=axes,
        )
        plt.tight_layout()
        plt.savefig(output_dir / "model_traceplot.png", dpi=150, bbox_inches="tight")
        plt.close()
        print(f"   Saved to: {output_dir / 'model_traceplot.png'}")
    except Exception as e:
        print(f"   Warning: Could not generate traceplot - {e}")

    # 3. åéªŒåˆ†å¸ƒå›¾ (Forest Plot for Industry Coefficients)
    print("\n[3] Generating Industry Effect Forest Plot...")
    try:
        plt.figure(figsize=(12, 8))
        az.plot_forest(trace, var_names=["beta_ind"], combined=True)
        plt.title("Impact of Industry on Contestant Strength (beta_industry)")
        plt.tight_layout()
        plt.savefig(
            output_dir / "industry_effect_forest.png", dpi=150, bbox_inches="tight"
        )
        plt.close()
        print(f"   Saved to: {output_dir / 'industry_effect_forest.png'}")
    except Exception as e:
        print(f"   Warning: Could not generate forest plot - {e}")

    # 4. èµ›å­£è¶‹åŠ¿å¯è§†åŒ–
    print("\n[4] Generating Season Trend Plot...")
    try:
        season_trend_post = (
            trace.posterior["season_trend"].mean(dim=["chain", "draw"]).values
        )
        season_trend_std = (
            trace.posterior["season_trend"].std(dim=["chain", "draw"]).values
        )

        inv_season_map = {v: k for k, v in season_map.items()}
        seasons_list = [inv_season_map[i] for i in range(len(season_map))]

        plt.figure(figsize=(12, 6))
        plt.errorbar(
            seasons_list,
            season_trend_post,
            yerr=season_trend_std,
            marker="o",
            linestyle="-",
            color="purple",
            capsize=5,
            capthick=2,
            linewidth=2,
            markersize=8,
        )
        plt.fill_between(
            seasons_list,
            season_trend_post - season_trend_std,
            season_trend_post + season_trend_std,
            alpha=0.3,
            color="purple",
        )
        plt.title("Season Baseline Trend (Random Walk)", fontsize=14)
        plt.xlabel("Season", fontsize=12)
        plt.ylabel("Baseline Strength Correction", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_dir / "season_trend.png", dpi=150, bbox_inches="tight")
        plt.close()
        print(f"   Saved to: {output_dir / 'season_trend.png'}")
    except Exception as e:
        print(f"   Warning: Could not generate season trend plot - {e}")

    # 5. é€‰æ‰‹æ’åè¡¨ (Top 30 by Average Vote Intensity)
    print("\n[5] Generating Top Contestants Bar Chart...")
    try:
        avg_strength = (
            result_df.groupby(["contestant_id", "celebrity_name"])[
                "vote_intensity_mean"
            ]
            .mean()
            .reset_index()
        )
        top_30 = avg_strength.sort_values("vote_intensity_mean", ascending=False).head(
            30
        )

        plt.figure(figsize=(14, 10))
        sns.barplot(
            x="vote_intensity_mean",
            y="celebrity_name",
            data=top_30,
            palette="viridis",
        )
        plt.title("Top 30 Contestants by Estimated Vote Intensity", fontsize=14)
        plt.xlabel("Average Vote Intensity Score", fontsize=12)
        plt.ylabel("Celebrity Name", fontsize=12)
        plt.tight_layout()
        plt.savefig(output_dir / "top_contestants.png", dpi=150, bbox_inches="tight")
        plt.close()
        print(f"   Saved to: {output_dir / 'top_contestants.png'}")

        # ä¿å­˜æ’åè¡¨åˆ°CSV
        avg_strength_sorted = avg_strength.sort_values(
            "vote_intensity_mean", ascending=False
        )
        avg_strength_sorted["rank"] = range(1, len(avg_strength_sorted) + 1)
        avg_strength_sorted.to_csv(output_dir / "contestant_ranking.csv", index=False)
        print(f"   Ranking saved to: {output_dir / 'contestant_ranking.csv'}")
    except Exception as e:
        print(f"   Warning: Could not generate top contestants chart - {e}")

    # 6. åéªŒé¢„æµ‹æ£€éªŒ (Posterior Predictive Check)
    print("\n[6] Generating Posterior Distribution Plots...")
    try:
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # beta_judge åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["beta_judge"], ax=axes[0, 0])
        axes[0, 0].set_title("beta_judge (Judge Score Weight)")

        # beta_week åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["beta_week"], ax=axes[0, 1])
        axes[0, 1].set_title("beta_week (Week Effect)")

        # theta åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["theta"], ax=axes[0, 2])
        axes[0, 2].set_title("theta (Base Popularity)")

        # theta_popularity åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["theta_popularity"], ax=axes[1, 0])
        axes[1, 0].set_title("theta_popularity (Low-Score Advance Effect)")

        # phi åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["phi"], ax=axes[1, 1])
        axes[1, 1].set_title("phi (Gamma Dispersion)")

        # beta_age åéªŒåˆ†å¸ƒ
        az.plot_posterior(trace, var_names=["beta_age"], ax=axes[1, 2])
        axes[1, 2].set_title("beta_age (Age Effect)")

        plt.tight_layout()
        plt.savefig(
            output_dir / "posterior_distributions.png", dpi=150, bbox_inches="tight"
        )
        plt.close()
        print(f"   Saved to: {output_dir / 'posterior_distributions.png'}")
    except Exception as e:
        print(f"   Warning: Could not generate posterior plots - {e}")

    # 7. R-hat å’Œ ESS è¯Šæ–­
    print("\n[7] Convergence Diagnostics (R-hat & ESS)...")
    try:
        # æ£€æŸ¥æ›´å¤šå‚æ•°çš„æ”¶æ•›æ€§ï¼ˆæ’é™¤å¯èƒ½ä¸ºç©ºçš„å‚æ•°ï¼‰
        var_names_to_check = [
            "beta_judge",
            "beta_week",
            "theta",
            "theta_popularity",
            "phi",
            "sigma_season",
            "sigma_alpha",
            "beta_age",
        ]

        # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨ä¸”æœ‰æ•°æ®çš„å˜é‡
        available_vars = []
        for var in var_names_to_check:
            if var in trace.posterior:
                var_data = trace.posterior[var]
                # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ•°ç»„
                if var_data.size > 0:
                    available_vars.append(var)

        if not available_vars:
            print("   âš ï¸  Warning: No variables available for convergence diagnostics")
            return

        rhat = az.rhat(trace, var_names=available_vars)
        ess = az.ess(trace, var_names=available_vars)

        print("\n   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("   â•‘  R-hat è¯Šæ–­ (åº”è¯¥æ¥è¿‘ 1.0, å»ºè®® < 1.01)              â•‘")
        print("   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        rhat_issues = []
        for var in rhat.data_vars:
            try:
                rhat_val = float(
                    rhat[var].values.flat[0]
                    if rhat[var].values.size > 0
                    else float("nan")
                )
                if not np.isnan(rhat_val):
                    status = (
                        "âœ…" if rhat_val < 1.01 else "âš ï¸" if rhat_val < 1.05 else "âŒ"
                    )
                    print(f"      {status} {var:20s}: {rhat_val:.4f}")
                    if rhat_val >= 1.01:
                        rhat_issues.append((var, rhat_val))
            except (ValueError, IndexError):
                print(f"      âš ï¸  {var:20s}: Could not compute")
                continue

        if rhat_issues:
            print("\n   âš ï¸  è­¦å‘Šï¼šä»¥ä¸‹å‚æ•°çš„ R-hat > 1.01ï¼Œå¯èƒ½éœ€è¦æ›´å¤šé‡‡æ ·:")
            for var, val in rhat_issues:
                print(f"      - {var}: {val:.4f}")
        else:
            print("\n   âœ… æ‰€æœ‰å‚æ•°çš„ R-hat < 1.01ï¼Œæ”¶æ•›è‰¯å¥½ï¼")

        print("\n   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("   â•‘  æœ‰æ•ˆæ ·æœ¬é‡ (ESS) - è¶Šå¤§è¶Šå¥½                         â•‘")
        print("   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        total_samples = trace.posterior.sizes["chain"] * trace.posterior.sizes["draw"]

        for var in ess.data_vars:
            try:
                ess_val = float(
                    ess[var].values.flat[0] if ess[var].values.size > 0 else 0
                )
                if ess_val > 0:
                    ess_ratio = ess_val / total_samples
                    status = (
                        "âœ…" if ess_ratio > 0.1 else "âš ï¸" if ess_ratio > 0.01 else "âŒ"
                    )
                    print(
                        f"      {status} {var:20s}: {ess_val:7.0f} ({ess_ratio:5.1%} of total)"
                    )
            except (ValueError, IndexError):
                print(f"      âš ï¸  {var:20s}: Could not compute")
                continue

        # ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶
        diagnostics_data = []
        for var in rhat.data_vars:
            try:
                rhat_val = float(
                    rhat[var].values.flat[0]
                    if rhat[var].values.size > 0
                    else float("nan")
                )
                ess_val = float(
                    ess[var].values.flat[0] if ess[var].values.size > 0 else 0
                )
                diagnostics_data.append(
                    {
                        "parameter": var,
                        "rhat": rhat_val,
                        "ess": ess_val,
                        "total_samples": total_samples,
                        "ess_ratio": ess_val / total_samples if ess_val > 0 else 0,
                    }
                )
            except (ValueError, IndexError):
                continue

        if diagnostics_data:
            diagnostics_df = pd.DataFrame(diagnostics_data)
            diagnostics_df.to_csv(
                output_dir / "convergence_diagnostics.csv", index=False
            )
            print(
                f"\n   ğŸ“„ Diagnostics saved to: {output_dir / 'convergence_diagnostics.csv'}"
            )

    except Exception as e:
        print(f"   âš ï¸  Warning: Could not compute diagnostics - {e}")
        import traceback

        traceback.print_exc()

    print("\n" + "=" * 60)
    print("âœ… Visualization completed!")
    print("=" * 60)


def predict_eliminations(
    result_df: pd.DataFrame,
    df: pd.DataFrame,
    season_map: Dict,
    flat_idx_map: Dict,
    output_dir: str = None,
) -> pd.DataFrame:
    """
    æ‰§è¡Œæœ«å°¾æ·˜æ±°é¢„æµ‹åˆ¤åˆ«
    å€Ÿé‰´è‡ª è´å¶æ–¯åˆ†å±‚+mcmc.py çš„ predict_eliminations å‡½æ•°
    """
    if output_dir is None:
        output_dir = OUTPUT_DIR / "trained"
    output_dir = OUTPUT_DIR / "trained"

    print("\n" + "=" * 60)
    print("ğŸ¯ Elimination Prediction Analysis")
    print("=" * 60)

    inv_season_map = {v: k for k, v in season_map.items()}
    seasons_list = [inv_season_map[i] for i in range(len(season_map))]

    # å‡†å¤‡å®¹å™¨
    prediction_results = []
    correct_predictions = 0
    total_elimination_events = 0

    # æŒ‰èµ›å­£å’Œå‘¨æ¬¡åˆ†ç»„
    for season in df["season"].unique():
        s_df = df[df["season"] == season]
        season_idx = season_map[season]
        season_total_weeks = s_df["season_total_weeks"].iloc[0]

        for week in range(1, min(12, season_total_weeks)):
            # è·å–æœ¬å‘¨æ‰€æœ‰å‚èµ›é€‰æ‰‹
            week_contestants = []
            for _, row in s_df.iterrows():
                c_id = row["contestant_id"]
                if (c_id, week) not in flat_idx_map:
                    continue

                has_next_week = (c_id, week + 1) in flat_idx_map

                # ä»ç»“æœä¸­è·å–æœ¬å‘¨çš„æŠ•ç¥¨å¼ºåº¦
                vote_intensity = result_df[
                    (result_df["contestant_id"] == c_id) & (result_df["week"] == week)
                ]["vote_intensity_mean"].values

                if len(vote_intensity) == 0:
                    continue

                week_contestants.append(
                    {
                        "contestant_id": c_id,
                        "celebrity_name": row["celebrity_name"],
                        "vote_intensity": vote_intensity[0],
                        "has_next_week": has_next_week,
                        "placement": row["placement"],
                    }
                )

            if len(week_contestants) == 0:
                continue

            # å®é™…æ·˜æ±°è€…ï¼ˆä¸‹å‘¨æ²¡æ•°æ®ä¸”ä¸æ˜¯å† å†›ï¼‰
            actual_eliminated = [
                c
                for c in week_contestants
                if not c["has_next_week"] and c["placement"] > 1
            ]
            actual_survived = [c for c in week_contestants if c["has_next_week"]]

            if len(actual_eliminated) == 0 or len(actual_survived) == 0:
                continue

            total_elimination_events += len(actual_eliminated)

            # æ¨¡å‹é¢„æµ‹ï¼šæŒ‰æŠ•ç¥¨å¼ºåº¦æ’åºï¼Œæœ€ä½çš„åº”è¯¥è¢«æ·˜æ±°
            sorted_contestants = sorted(
                week_contestants, key=lambda x: x["vote_intensity"]
            )
            predicted_eliminated = sorted_contestants[: len(actual_eliminated)]

            # æ¯”å¯¹ç»“æœ
            actual_ids = set(c["contestant_id"] for c in actual_eliminated)
            predicted_ids = set(c["contestant_id"] for c in predicted_eliminated)
            hits = len(actual_ids.intersection(predicted_ids))
            correct_predictions += hits

            # è®°å½•è¯¦ç»†æ—¥å¿—
            prediction_results.append(
                {
                    "Season": season,
                    "Week": week,
                    "Actual_Eliminated": ", ".join(
                        str(c["celebrity_name"]) for c in actual_eliminated
                    ),
                    "Actual_Eliminated_IDs": list(actual_ids),
                    "Predicted_Eliminated": ", ".join(
                        str(c["celebrity_name"]) for c in predicted_eliminated
                    ),
                    "Predicted_Eliminated_IDs": list(predicted_ids),
                    "Correct_Count": hits,
                    "Total_Eliminated": len(actual_eliminated),
                    "Is_Correct": hits == len(actual_eliminated),
                }
            )

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = (
        correct_predictions / total_elimination_events
        if total_elimination_events > 0
        else 0
    )

    print(f"\nğŸ“ˆ Prediction Statistics:")
    print(f"   Total elimination events: {total_elimination_events}")
    print(f"   Correct predictions: {correct_predictions}")
    print(f"   Elimination prediction accuracy: {accuracy:.2%}")

    # è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
    pred_df = pd.DataFrame(prediction_results)
    output_path = output_dir / "elimination_predictions.csv"
    pred_df.to_csv(output_path, index=False)
    print(f"\nâœ… Detailed predictions saved to: {output_path}")

    # æŒ‰èµ›å­£ç»Ÿè®¡å‡†ç¡®ç‡
    if len(pred_df) > 0:
        print("\nğŸ“Š Accuracy by Season:")
        season_stats = (
            pred_df.groupby("Season")
            .agg(
                {
                    "Correct_Count": "sum",
                    "Total_Eliminated": "sum",
                }
            )
            .reset_index()
        )
        season_stats["Accuracy"] = (
            season_stats["Correct_Count"] / season_stats["Total_Eliminated"]
        )
        for _, row in season_stats.iterrows():
            print(
                f"   Season {row['Season']}: {row['Correct_Count']}/{row['Total_Eliminated']} = {row['Accuracy']:.2%}"
            )

    return pred_df


def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„è´å¶æ–¯ MCMC æ¨æ–­æµç¨‹"""

    INPUT_FILE = "preprocessed_data_percentage.csv"  # ä½¿ç”¨ percentage ç‰ˆæœ¬çš„æ•°æ®
    OUTPUT_FILE = "bayesian_vote_intensity.csv"
    MAX_WEEKS = 11

    n_cores = mp.cpu_count()
    # highlight: MCMC é…ç½®å¯ç‚¹
    mcmc_config = MCMCConfig(
        draws=400,  # ä¼˜åŒ–ï¼šä»500å¢è‡³1000
        tune=400,  # ä¼˜åŒ–ï¼šä»500å¢è‡³1000
        chains=min(n_cores, 4),
        cores=min(n_cores, 4),
        target_accept=0.85,  # ä¼˜åŒ–ï¼šä»0.9å¢è‡³0.95
        init="advi+adapt_diag",
    )

    print("=" * 60)
    print("ğŸ”¥ Bayesian Hierarchical Model + MCMC Inference")
    print(f"   Using {mcmc_config.cores} CPU cores (max available: {n_cores})")
    print("   C++ compilation enabled for acceleration")
    print("=" * 60)

    # [1/8] åŠ è½½æ•°æ®
    print("\n[1/8] Loading preprocessed data...")
    df = load_preprocessed_data(INPUT_FILE)

    # [2/8] å‡†å¤‡ç´¢å¼•
    print("\n[2/8] Preparing indices...")
    season_map, n_seasons, n_contestants = prepare_indices(df)
    print(f"      Seasons: {n_seasons}, Contestants: {n_contestants}")

    # [3/8] æå–ç‰¹å¾
    print("\n[3/8] Extracting features...")
    X_industry, X_age, X_advanced_rounds = extract_features(df, n_contestants)
    print(f"      Industry features: {X_industry.shape[1]}")
    print(
        f"      Advanced rounds range: [{X_advanced_rounds.min():.2f}, {X_advanced_rounds.max():.2f}] (standardized)"
    )

    # [4/8] æ„å»ºè§‚æµ‹æ•°æ®
    print("\n[4/8] Building observation data...")
    obs_season_idx, obs_week_idx, obs_contestant_idx, obs_score_sum, flat_idx_map = (
        build_observation_data(df, MAX_WEEKS)
    )
    n_observations = len(obs_score_sum)
    print(f"      Observations: {n_observations}")

    # [5/8] æ„å»ºæ·˜æ±°çº¦æŸ
    print("\n[5/8] Building elimination constraints...")
    elimination_pairs, pair_info = build_elimination_pairs(
        df, season_map, flat_idx_map, MAX_WEEKS
    )

    # æ‰“å°éªŒè¯ä¿¡æ¯ï¼ˆå‰5ä¸ªé…å¯¹ï¼‰
    if len(elimination_pairs) > 0:
        print(f"      Total pairs: {len(elimination_pairs)}")
        print(f"      Sample pairs (ä¸¥æ ¼é¿å…æœªæ¥ä¿¡æ¯æ³„éœ²):")
        for i in range(min(5, len(elimination_pairs))):
            info = pair_info[i]
            print(
                f"        Week {info['week']}, Season {info['season']}: "
                f"{info['winner']} (ä¸‹å‘¨ç»§ç»­) > {info['loser']} (ä¸‹å‘¨æ·˜æ±°)"
            )

    # é™åˆ¶çº¦æŸæ•°é‡ä»¥åŠ é€Ÿï¼ˆéšæœºé‡‡æ ·ï¼‰
    MAX_PAIRS = 200
    if len(elimination_pairs) > MAX_PAIRS:
        idx = np.random.choice(len(elimination_pairs), MAX_PAIRS, replace=False)
        elimination_pairs = elimination_pairs[idx]
        print(f"      Sampled pairs for efficiency: {len(elimination_pairs)}")

    # [6/8] æ„å»º PyMC æ¨¡å‹
    print("\n[6/8] Building PyMC model...")
    model = build_pymc_model(
        obs_season_idx=obs_season_idx,
        obs_week_idx=obs_week_idx,
        obs_contestant_idx=obs_contestant_idx,
        obs_score_sum=obs_score_sum,
        X_industry=X_industry,
        X_age=X_age,
        X_advanced_rounds=X_advanced_rounds,
        elimination_pairs=elimination_pairs,
        n_seasons=n_seasons,
        n_contestants=n_contestants,
        n_observations=n_observations,
    )
    print("      Model built successfully!")

    # [7/8] è¿è¡Œ MCMC é‡‡æ ·
    print("\n[7/8] Running MCMC sampling...")
    trace = run_mcmc_sampling(model, mcmc_config)
    print("      Sampling completed!")

    # [8/8] æå–å¹¶ä¿å­˜ç»“æœ
    print("\n[8/8] Extracting and saving results...")
    result_df = extract_results(
        trace=trace,
        df=df,
        obs_season_idx=obs_season_idx,
        obs_week_idx=obs_week_idx,
        obs_contestant_idx=obs_contestant_idx,
        obs_score_sum=obs_score_sum,
        season_map=season_map,
    )
    save_results(result_df, OUTPUT_FILE)

    # [9/11] å¯¼å‡ºæ·˜æ±°/æ™‹çº§åˆ†æåˆ°Excel
    print("\n[9/11] Exporting elimination analysis to Excel...")
    export_elimination_analysis_to_excel(
        df=df,
        flat_idx_map=flat_idx_map,
        pair_info=pair_info,
        output_file="elimination_analysis.xlsx",
    )

    # [10/11] æ¨¡å‹è¯Šæ–­ä¸å¯è§†åŒ–
    print("\n[10/11] Model diagnostics and visualization...")
    analyze_and_visualize_results(
        trace=trace,
        result_df=result_df,
        df=df,
        season_map=season_map,
    )

    # [11/11] æ·˜æ±°é¢„æµ‹åˆ†æ
    print("\n[11/11] Elimination prediction analysis...")
    pred_df = predict_eliminations(
        result_df=result_df,
        df=df,
        season_map=season_map,
        flat_idx_map=flat_idx_map,
    )

    print("\n" + "=" * 60)
    print("ğŸ‰ Bayesian MCMC inference completed!")
    print(f"   Total rows: {len(result_df)}")
    print("=" * 60)

    # æ‰“å°ç¤ºä¾‹ç»“æœ
    print("\nğŸ“Š Sample results (first 10 rows):")
    print(
        result_df[
            [
                "celebrity_name",
                "season",
                "week",
                "judge_score_sum",
                "vote_intensity_mean",
            ]
        ]
        .head(10)
        .to_string(index=False)
    )

    # è¿”å›ç»“æœä¾›è¿›ä¸€æ­¥åˆ†æ
    return {
        "result_df": result_df,
        "trace": trace,
        "df": df,
        "season_map": season_map,
        "pred_df": pred_df,
    }


if __name__ == "__main__":
    results = main()
